{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tokenization.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyHJy9LZkKbE"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5iriFBEeeDl"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXOoYO1femJF"
      },
      "source": [
        "Setelah mengimport library kita membuat sebuah objek tokenizer dengan memanggil fungsi tokenizer dan melengkapi parameternya.\n",
        "\n",
        "> Parameter num_words adalah jumlah kata yang akan dikonversi ke dalam token/bilangan numerik\n",
        "\n",
        "> paramter oov adalah parameter yang berfungsi untuk mengganti kata-kata yang tidak ditokenisasi menjadi karakter tertentu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29SrV_u2fSge"
      },
      "source": [
        "tokenizer = Tokenizer(num_words = 15, oov_token = '-')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egMCTgBffaXH"
      },
      "source": [
        "# Membuat sebuah teks yang akan kita tokenisasi\n",
        "teks = [\n",
        "        'Saya suka programming',\n",
        "        'Programming sangat menyenangkan!',\n",
        "        'Machine Learning berbeda dengan pemrograman konvensional'\n",
        "]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHpwK6KhfpP1"
      },
      "source": [
        "Untuk melakukan tokenisasi, panggil fungsi fit_on_text pada objek tokenizer. Isi teks adalah argumennya"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMV1BbjffxMk"
      },
      "source": [
        "tokenizer.fit_on_texts(teks)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cIKEMfGf04Z"
      },
      "source": [
        "Mengubah kalimat ke dalam nilai yang sesuai dengan fungsi texts_to_sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hbefeMIf6sq"
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(teks)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8E8SkpmgEze"
      },
      "source": [
        "Untuk melihat hasil tokenisasi, panggil atribut word_index dari objek tokenizer. Atribut word index mengembalikan dictionary berupa kata sebagai key dan token sebagai value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zlKCLWOgXnn",
        "outputId": "d6d6514c-7943-4961-a416-b778a006f4e1"
      },
      "source": [
        "print(tokenizer.word_index)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'-': 1, 'programming': 2, 'saya': 3, 'suka': 4, 'sangat': 5, 'menyenangkan': 6, 'machine': 7, 'learning': 8, 'berbeda': 9, 'dengan': 10, 'pemrograman': 11, 'konvensional': 12}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoV0AhejhXqT"
      },
      "source": [
        "Untuk mengubah kalimat ke dalam nilai-nilai yang sesuai dapat dengan menggunakan fungsi text_to_sequence. Hal yang selanjutnya kita perlukan adalah padding. Padding adalah proses untuk membuat setiap kalimat pada teks memiliki panjang yang seragam. Untuk menggunakan padding import library pad_sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47IGY4fJhyk_",
        "outputId": "1eb590ac-96b6-4aaf-f4da-12acd81dd51f"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "sequences_samapanjang = pad_sequences(sequences)\n",
        "\n",
        "print(sequences_samapanjang)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  0  0  3  4  2]\n",
            " [ 0  0  0  2  5  6]\n",
            " [ 7  8  9 10 11 12]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGYL222njROC"
      },
      "source": [
        "Jika ingin mengubah sehingga 0 berada di akhir sequence, kita dapat menggunakan paramter padding dengan nilai post. Selain itu kita dapat mengatur berapa maksimum panjang setiap sequence dengan parameter maxlen, ketika kita memberi paramter maxlen maka secara default nilai dari sequence diambil dari n nilai terakhir atau n kata terakhir saja dari setiap kalimat. Untuk mengubah pengaturan ini dan mengambil n kata terakhir dari tiap kalimat bisa menggunakan parameter truncating dan mengisi nilai 'post'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHsdpoFMjhST"
      },
      "source": [
        "sequences_samapanjang = pad_sequences(\n",
        "    sequences,\n",
        "    padding='post',\n",
        "    maxlen=5,\n",
        "    truncating='post'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}